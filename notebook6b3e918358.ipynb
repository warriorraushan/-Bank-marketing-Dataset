{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the basic libraries\n\nimport numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\nimport scikitplot as skplt\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset\n\ndataset = pd.read_csv('/kaggle/input/bank-additional-full.csv', sep = ';', na_values = 'unknown')\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the dataset\n\nprint(dataset.shape)\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Descriptive Statistics\n\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for missing values. The dataset specifies a token 'unknown' which has been used to mark the missing values\n\ndataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Condition to extract columns that contain the missing values\n\nmiss_cond = np.array(dataset.isnull().sum() != 0)\nmiss_col_ind = np.where(miss_cond)[0]\nmiss_cols = list(dataset.columns[miss_col_ind])\n\nprint(miss_col_ind)\nprint(miss_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list to store all the unique values and their count from the columns that contains missing values\n\nmissing_column_freq = []\n\nfor i in miss_col_ind:\n    missing_column_freq.append(dataset.iloc[:, i].value_counts())\n    \nmissing_column_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a fresh dataset to store only a fragment of the original dataset that needs preprocessing\n\ndataset_miss = dataset[miss_cols]\ndataset_miss.isnull().sum()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending 'Nan' and its count in the missing colum frequency list. \n\nfor i in range(6):\n    missing_column_freq[i] = missing_column_freq[i].append(pd.Series({'null' : dataset_miss.isnull().sum()[i]}))\n\nmissing_column_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing all the null values with respect to the legitimate values for the columns that contain 'NaN'\n\nfor i in range(6):\n    plt.figure(figsize = (18, 5))\n    sns.barplot(missing_column_freq[i].index, missing_column_freq[i].values)\n    plt.savefig(\"figure {}\".format(i))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting an object of Simple Imputer which intends to replace the NaN values with the mode of a particular attribute\n\nfrom sklearn.impute import SimpleImputer\nsim = SimpleImputer(strategy = \"most_frequent\")\nsim.fit(dataset_miss)\nprint('NaN values shall be replaced by the following values : ', sim.statistics_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the NaN values and double checking the results\n\ndataset_miss = pd.DataFrame(sim.transform(dataset_miss))\ndataset_miss.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a copy of the original dataset and replacing the NaN fragment with the Preprocessed fragment\n\ndataset1 = dataset.copy()\ndataset1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the NaN fragment with the Preprocessed fragment\n\ndataset1[miss_cols] = dataset_miss\ndataset1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing all the integer columns in a separate dataset\n\ndataset_with_integers = dataset.select_dtypes(exclude = ['O'])\ndataset_with_integers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing EDA (Exploratory Data Analysis)\n\nlen = dataset_with_integers.shape[1]\nlen\n\ncols = list(dataset_with_integers.columns)\ncols\n\nfor i in range(len):\n    plt.figure(figsize = (18, 5))\n    plt.hist(dataset_with_integers[cols[i]], bins = 100)\n    plt.xlabel(cols[i])\n    plt.ylabel('Frequency')\n    plt.title('Histogram Analysis for {} variable'.format(cols[i]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting scatter-matrix\n\npd.plotting.scatter_matrix(dataset1, figsize = (18, 15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting correlation matrix to identify important columns\n\ncorr_mat = dataset1.corr()\ncorr_mat_sp = dataset1.corr('spearman')\ncorr_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the heatmap\n\nplt.figure(figsize = (18, 8))\nsns.heatmap(corr_mat, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a new copy of the preprocessed dataset and dropping the irrelevant columns\n\ndataset2 = dataset1.copy()\ndataset2 = dataset2.drop(['duration', 'pdays'], axis = 1)\ndataset2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing Feature Scaling\n\nscaler = StandardScaler()\ndataset_with_integers = scaler.fit_transform(dataset_with_integers)\ndataset_with_integers = pd.DataFrame(dataset_with_integers)\ndataset_with_integers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performing One Hot Encoding on the categorical variables\n\ndataset_str = dataset2.select_dtypes(include = ['O'])\ndataset_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the get_dummies() function of pandas\n\ndataset_str = pd.get_dummies(dataset_str)\ndataset_str","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating both the preprocessed dataframes with integer and categorical variables respectively\n\nfinal_dataset = pd.concat([dataset_with_integers, dataset_str], axis = 1)\nprint(final_dataset.shape)\nfinal_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slicing the preprocessed dataset into feature matrix (X) and vector of predictions (y)\n\nX = final_dataset.iloc[:, :-2].values\ny = final_dataset.iloc[:, -2:].values\ny = np.argmax(y, axis = 1)\nX, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into training set and test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(solver = 'saga', max_iter = 1000)\nlog_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the training set and test set performance\n\nprint(\"Training Set : \", log_reg.score(X_train, y_train))\nprint(\"Testing Set : \", log_reg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying K-fold Cross Validation\n\ncross_val_score(log_reg, X_train, y_train, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the decision score for logistic function\n\ny_log_scores = cross_val_predict(log_reg, X_train, y_train, cv = 3, method = 'decision_function')\ny_log_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the Precision, Recall, F1 Score\n\ny_log_pred = cross_val_predict(log_reg, X_train, y_train, cv = 3)\n\nprint('Precision : ', precision_score(y_train, y_log_pred))\nprint('Recall : ', recall_score(y_train, y_log_pred))\nprint('F1 Score : ', f1_score(y_train, y_log_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the confusion matrix\n\nskplt.metrics.plot_confusion_matrix(y_train, y_log_pred, figsize=(8, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing the parameters for a Precision Recall Curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_log_scores)\nprecision, recall, threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precision Recall Curve\n\nplt.figure(figsize = (18, 5))\nplt.plot(threshold, precision[:-1], c = \"r\", label = 'Precision')\nplt.plot(threshold, recall[:-1], c = \"g\", label = 'Recall')\nplt.xlabel('Threshold')\nplt.title('Analyzing precision and recall changes')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Precision vs Recall\n\nplt.figure(figsize = (18, 5))\nplt.plot(recall, precision)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision vs Recall')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve (Sensitivity vs 1-Specificity)\n\nfpr, tpr, th = roc_curve(y_train, y_log_scores)\n\nplt.figure(figsize = (18, 7))\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate (Sensitivity)')\nplt.ylabel('True Positive Rate (1-Specificity)')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the AUC Score\n\nroc_auc_score(y_train, y_log_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the Random Forest Algorithm\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the training set and test set performance\n\nprint(\"Training Set : \", rf.score(X_train, y_train))\nprint(\"Testing Set : \", rf.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying K-fold Cross Validation\n\ncross_val_score(rf, X_train, y_train, cv = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the decision score for random forest\n\ny_proba_rf = cross_val_predict(rf, X_train, y_train, cv = 5, method = 'predict_proba')\ny_scores_forest = y_proba_rf[:, 1]\ny_scores_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the Precision, Recall, F1 Score\n\ny_rf_pred = cross_val_predict(rf, X_train, y_train, cv = 3)\n\nprint('Precision : ', precision_score(y_train, y_rf_pred))\nprint('Recall : ', recall_score(y_train, y_rf_pred))\nprint('F1 Score : ', f1_score(y_train, y_rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the confusion matrix\n\nskplt.metrics.plot_confusion_matrix(y_train, y_rf_pred, figsize=(8, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing the parameters for a Precision Recall Curve\n\nprecision_rf, recall_rf, threshold_rf = precision_recall_curve(y_train, y_scores_forest)\nprecision_rf, recall_rf, threshold_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Precision Recall Curve\n\nplt.figure(figsize = (18, 5))\nplt.plot(threshold_rf, precision_rf[:-1], c = \"r\", label = 'Precision')\nplt.plot(threshold_rf, recall_rf[:-1], c = \"g\", label = 'Recall')\nplt.xlabel('Threshold')\nplt.title('Analyzing precision and recall changes')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Precision vs Recall\n\nplt.figure(figsize = (18, 5))\nplt.plot(recall_rf, precision_rf)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision vs Recall')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the ROC Curve (Sensitivity vs 1-Specificity)\n\nfpr_forest, tpr_forest, th_forest = roc_curve(y_train, y_scores_forest)\n\nplt.figure(figsize = (18, 7))\nplt.plot(fpr_forest, tpr_forest)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate (Sensitivity)')\nplt.ylabel('True Positive Rate (1-Specificity)')\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the AUC Score\n\nroc_auc_score(y_train, y_scores_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing the ROC Curve for Logistic Regression and Random Forest\n\nplt.figure(figsize = (18, 7))\nplt.plot(fpr, tpr, c = 'g')\nplt.plot(fpr_forest, tpr_forest, c = 'r')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate (Sensitivity)')\nplt.ylabel('True Positive Rate (1-Specificity)')\nplt.legend()\nplt.title('ROC Curve')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the DL libraries\n\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of the dataset\n\nX_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the model\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(128, activation = 'tanh', input_shape = [57, ]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation = 'tanh'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(32, activation = 'tanh'), \n    keras.layers.Dense(32, activation = 'tanh'), \n    keras.layers.Dense(2, activation = 'softmax')\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compiling the model\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the shape of y\n\ny_train = to_categorical(y_train)\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the dataset into training set and validation set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.05)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model\n\nhistory = model.fit(X_train, y_train, epochs = 20, validation_data = (X_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing the results\n\npd.DataFrame(history.history).plot(figsize = (12, 6))\nplt.gca().set_ylim(0, 1)\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the shape of the test set label\n\ny_test = to_categorical(y_test)\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation on the test set\n\ntest_loss, test_accuracy = model.evaluate(X_test, y_test)\nprint('Test set loss : ', test_loss)\nprint('Test set accuracy : ', test_accuracy)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}